{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DocumentEmbedder:\n",
    "    def __init__(self, model_name='bert-large-uncased'):\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = BertModel.from_pretrained(model_name)\n",
    "    def embed_doc(self, documents):\n",
    "        # Initialize an empty list to store the document embeddings\n",
    "        document_embeddings = []\n",
    "\n",
    "        # Loop through each document and embed it\n",
    "        for doc in documents:\n",
    "            # Tokenize the document and convert it to tensors\n",
    "            inputs = self.tokenizer(doc, return_tensors='pt', truncation=True, padding=True)\n",
    "            \n",
    "            # Forward pass through the BERT model\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                # Extract the last layer embeddings for each token\n",
    "                embeddings = outputs.last_hidden_state\n",
    "            \n",
    "            # Calculate the mean of token embeddings to get document-level embedding\n",
    "            doc_embedding = torch.mean(embeddings, dim=1).squeeze().numpy()\n",
    "            document_embeddings.append(doc_embedding)\n",
    "        return document_embeddings\n",
    "    \n",
    "    def encode_question(self, question, max_length=128):\n",
    "        tokens = self.tokenizer.encode(question, add_special_tokens=True, max_length=max_length, truncation=True)\n",
    "\n",
    "        # Convert tokens to tensors\n",
    "        input_ids = torch.tensor([tokens])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids)\n",
    "            question_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        # Convert the PyTorch tensor to a NumPy array\n",
    "        question_embedding_np = question_embedding.numpy()\n",
    "\n",
    "        return question_embedding_np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = DocumentEmbedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from serpapi import GoogleSearch\n",
    "\n",
    "def get_query(query):\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"engine\": \"google\",\n",
    "        \"api_key\": \"d5283d8a6c6640c36e5228ae57e8baa9170859f8b5fa73e3c941cdb51afa9e0f\"\n",
    "    }\n",
    "\n",
    "    search = GoogleSearch(params)\n",
    "    results = search.get_dict()\n",
    "    organic_results = results['organic_results']\n",
    "    return organic_results\n",
    "\n",
    "organic_results = get_query(\"Professor Lawrence Angrave UIUC\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trafilatura as tr\n",
    "import pandas as pd\n",
    "\n",
    "def parse_results(link):\n",
    "    downloaded = tr.fetch_url(link)\n",
    "    text = tr.extract(downloaded, include_formatting=True)\n",
    "    if text == None:\n",
    "        return ''\n",
    "    return text\n",
    "\n",
    "def get_scholar_results(author_id):\n",
    "    params = {\n",
    "        'engine': 'google_scholar_author',\n",
    "        'author_id': author_id,\n",
    "        'api_key' : 'd5283d8a6c6640c36e5228ae57e8baa9170859f8b5fa73e3c941cdb51afa9e0f'\n",
    "    }\n",
    "    search = GoogleSearch(params)\n",
    "    results = search.get_dict()\n",
    "    return results\n",
    "\n",
    "dataset = {'titles': [], 'text': []}\n",
    "\n",
    "for result in organic_results:\n",
    "    if 'scholar.google.com' in result['link']:\n",
    "        # get user id from link\n",
    "        user_id = result['link'].split('user=')[1].split('&')[0]\n",
    "        # get scholar results\n",
    "        scholar_results = get_scholar_results(user_id)\n",
    "        print(scholar_results[\"author\"])\n",
    "    else:\n",
    "        with open(\"Documents/\" + result['title'] + '.txt', 'w') as f:\n",
    "            f.write(parse_results(result['link']))\n",
    "        dataset['titles'].append(result['title'])\n",
    "        dataset['text'].append(parse_results(result['link']))\n",
    "\n",
    "df = pd.DataFrame(dataset)\n",
    "# df.to_csv('Dataset.csv', index=False, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the text into passages\n",
    "\n",
    "def split_text(text: str, n=100, character=\" \"):\n",
    "    \"\"\"Split the text every ``n``-th occurrence of ``character``\"\"\"\n",
    "    text = text.split(character)\n",
    "    return [character.join(text[i : i + n]).strip() for i in range(0, len(text), n)]\n",
    "\n",
    "\n",
    "def split_documents(documents: dict) -> dict:\n",
    "    \"\"\"Split documents into passages\"\"\"\n",
    "    titles, texts = [], []\n",
    "    for title, text in zip(documents[\"titles\"], documents[\"text\"]):\n",
    "        if text is not None:\n",
    "            for passage in split_text(text):\n",
    "                titles.append(title if title is not None else \"\")\n",
    "                texts.append(passage)\n",
    "    return {\"title\": titles, \"text\": texts}\n",
    "\n",
    "documents = df.to_dict('list')\n",
    "documents = split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "documents[\"embedding\"] = bert.embed_doc(documents[\"text\"])\n",
    "print(len(documents[\"embedding\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_embed = pd.DataFrame(documents)\n",
    "print(new_df_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_id = {label: i for i, label in enumerate(new_df_embed['title'].unique())}\n",
    "print(label_to_id)\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_embed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "train_features = np.array(new_df_embed['embedding'].tolist())\n",
    "train_labels = np.array([label_to_id[i] for i in new_df_embed['title'].tolist()])\n",
    "print(\"train_features shape = \", train_features.shape)\n",
    "print(\"train_labels shape = \", train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create a KNN classifier with cosine similarity as the metric\n",
    "knn = KNeighborsClassifier(metric='cosine')\n",
    "\n",
    "# Fit the KNN model with document embeddings\n",
    "knn.fit(train_features, train_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What Courses has professor Lawrence Angrave taught?\"\n",
    "question_embedding = bert.encode_question(question)[0]\n",
    "print(question_embedding.shape)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "def get_answer(question, context):\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "    model_flan = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\",max_length=2048)\n",
    "    input_text = \"Answer based on context \\n\\n context: v1 \\n\\n question: v2\" \n",
    "    input_text = input_text.replace('v1', context)\n",
    "    input_text = input_text.replace('v2', question)\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "    outputs = model_flan.generate(input_ids)\n",
    "    return tokenizer.decode(outputs[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# get the top k most relevant documents\n",
    "k = 2\n",
    "top_k = knn.predict_proba([question_embedding])[0].argsort()[-k:][::-1]\n",
    "\n",
    "for i in top_k:\n",
    "    print(id_to_label[i])\n",
    "\n",
    "\n",
    "\n",
    "# concatenate the text from the top k documents\n",
    "context = df.iloc[top_k]['text'].str.cat(sep=' ')\n",
    "# print(context)\n",
    "\n",
    "\n",
    "\n",
    "# get the answer\n",
    "answer = get_answer(question, context)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the documents folder\n",
    "import os\n",
    "\n",
    "for file in os.listdir('Documents'):\n",
    "    os.remove('Documents/' + file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
