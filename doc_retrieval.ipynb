{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DocumentEmbedder:\n",
    "    def __init__(self, model_name='bert-large-uncased'):\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = BertModel.from_pretrained(model_name)\n",
    "    def embed_doc(self, documents):\n",
    "        # Initialize an empty list to store the document embeddings\n",
    "        document_embeddings = []\n",
    "\n",
    "        # Loop through each document and embed it\n",
    "        for doc in documents:\n",
    "            # Tokenize the document and convert it to tensors\n",
    "            inputs = self.tokenizer(doc, return_tensors='pt', truncation=True, padding=True)\n",
    "            \n",
    "            # Forward pass through the BERT model\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                # Extract the last layer embeddings for each token\n",
    "                embeddings = outputs.last_hidden_state\n",
    "            \n",
    "            # Calculate the mean of token embeddings to get document-level embedding\n",
    "            doc_embedding = torch.mean(embeddings, dim=1).squeeze().numpy()\n",
    "            document_embeddings.append(doc_embedding)\n",
    "        return document_embeddings\n",
    "    \n",
    "    def encode_question(self, question, max_length=128):\n",
    "        tokens = self.tokenizer.encode(question, add_special_tokens=True, max_length=max_length, truncation=True)\n",
    "\n",
    "        # Convert tokens to tensors\n",
    "        input_ids = torch.tensor([tokens])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids)\n",
    "            question_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        # Convert the PyTorch tensor to a NumPy array\n",
    "        question_embedding_np = question_embedding.numpy()\n",
    "\n",
    "        return question_embedding_np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = DocumentEmbedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from serpapi import GoogleSearch\n",
    "\n",
    "def get_query(query):\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"engine\": \"google\",\n",
    "        \"api_key\": \"d5283d8a6c6640c36e5228ae57e8baa9170859f8b5fa73e3c941cdb51afa9e0f\"\n",
    "    }\n",
    "\n",
    "    search = GoogleSearch(params)\n",
    "    results = search.get_dict()\n",
    "    organic_results = results['organic_results']\n",
    "    return organic_results\n",
    "\n",
    "Query = \"Inception movie\"\n",
    "organic_results = get_query(Query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get named entities form the Query variable\n",
    "\n",
    "import spacy\n",
    "\n",
    "def get_named_entities(query):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    # get the named entities from the query\n",
    "    # return a list of named entities\n",
    "    # example: ['Michael Jordan', 'Chicago Bulls', 'NBA']\n",
    "    named_entities = []\n",
    "    doc = nlp(query)\n",
    "    for ent in doc.ents:\n",
    "        named_entities.append(ent.text)\n",
    "    return named_entities\n",
    "\n",
    "print(get_named_entities(\"Professor Lawrence Angrave\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trafilatura as tr\n",
    "import pandas as pd\n",
    "import wikipedia as wiki\n",
    "\n",
    "def parse_results(link):\n",
    "    downloaded = tr.fetch_url(link)\n",
    "    text = tr.extract(downloaded, include_formatting=True)\n",
    "    if text == None:\n",
    "        return ''\n",
    "    return text\n",
    "\n",
    "def get_scholar_results(author_id):\n",
    "    params = {\n",
    "        'engine': 'google_scholar_author',\n",
    "        'author_id': author_id,\n",
    "        'api_key' : 'd5283d8a6c6640c36e5228ae57e8baa9170859f8b5fa73e3c941cdb51afa9e0f'\n",
    "    }\n",
    "    search = GoogleSearch(params)\n",
    "    results = search.get_dict()\n",
    "    return results\n",
    "\n",
    "dataset = {'titles': [], 'text': []}\n",
    "\n",
    "named_entity = \"Inception\"\n",
    "for result in organic_results:\n",
    "    if 'wikipedia.org' in result['link']:\n",
    "        with open(\"Documents/wiki.txt\", 'w') as f:\n",
    "            wiki_results = wiki.search(result['title'])\n",
    "            page = wiki.page(wiki_results[0], auto_suggest=False)\n",
    "            f.write(page.content)\n",
    "        dataset['titles'].append(\"wikiresult\")\n",
    "        dataset['text'].append(page.content)\n",
    "            \n",
    "    # if 'scholar.google.com' in result['link']:\n",
    "    #     # get user id from link\n",
    "    #     user_id = result['link'].split('user=')[1].split('&')[0]\n",
    "    #     # get scholar results\n",
    "    #     scholar_results = get_scholar_results(user_id)\n",
    "    #     print(scholar_results[\"author\"])\n",
    "    else:\n",
    "        text = parse_results(result['link'])\n",
    "        if named_entity.lower() in text.lower():\n",
    "            with open(\"Documents/\" + result['title'] + '.txt', 'w') as f:\n",
    "                f.write(parse_results(result['link']))\n",
    "            dataset['titles'].append(result['title'])\n",
    "            dataset['text'].append(parse_results(result['link']))\n",
    "\n",
    "df = pd.DataFrame(dataset)\n",
    "# df.to_csv('Dataset.csv', index=False, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the text into passages\n",
    "\n",
    "def split_text(text: str, n=100, character=\" \"):\n",
    "    \"\"\"Split the text every ``n``-th occurrence of ``character``\"\"\"\n",
    "    text = text.split(character)\n",
    "    return [character.join(text[i : i + n]).strip() for i in range(0, len(text), n)]\n",
    "\n",
    "\n",
    "def split_documents(documents: dict) -> dict:\n",
    "    \"\"\"Split documents into passages\"\"\"\n",
    "    titles, texts = [], []\n",
    "    for title, text in zip(documents[\"titles\"], documents[\"text\"]):\n",
    "        if text is not None:\n",
    "            for passage in split_text(text):\n",
    "                titles.append(title if title is not None else \"\")\n",
    "                texts.append(passage)\n",
    "    return {\"title\": titles, \"text\": texts}\n",
    "\n",
    "documents = df.to_dict('list')\n",
    "documents = split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "documents[\"embedding\"] = bert.embed_doc(documents[\"text\"])\n",
    "print(len(documents[\"embedding\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_embed = pd.DataFrame(documents)\n",
    "print(new_df_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_id = {label: i for i, label in enumerate(new_df_embed['title'].unique())}\n",
    "print(label_to_id)\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_embed.head(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "train_features = np.array(new_df_embed['embedding'].tolist())\n",
    "train_labels = np.array([label_to_id[i] for i in new_df_embed['title'].tolist()])\n",
    "print(\"train_features shape = \", train_features.shape)\n",
    "print(\"train_labels shape = \", train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create a KNN classifier with cosine similarity as the metric\n",
    "knn = KNeighborsClassifier(metric='cosine')\n",
    "\n",
    "# Fit the KNN model with document embeddings\n",
    "knn.fit(train_features, train_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"When was inception released\"\n",
    "question_embedding = bert.encode_question(question)[0]\n",
    "print(question_embedding.shape)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "def get_answer(question, context):\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "    model_flan = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\",max_length=2048)\n",
    "    input_text = \"Answer based on context \\n\\n context: v1 \\n\\n question: v2\" \n",
    "    input_text = input_text.replace('v1', context)\n",
    "    input_text = input_text.replace('v2', question)\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "    outputs = model_flan.generate(input_ids)\n",
    "    return tokenizer.decode(outputs[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# get the top k most relevant documents\n",
    "k = 2\n",
    "top_k = knn.predict_proba([question_embedding])[0].argsort()[-k:][::-1]\n",
    "\n",
    "for i in top_k:\n",
    "    print(id_to_label[i])\n",
    "\n",
    "\n",
    "\n",
    "# concatenate the text from the top k documents\n",
    "context = df.iloc[top_k]['text'].str.cat(sep=' ')\n",
    "# print(context)\n",
    "\n",
    "\n",
    "\n",
    "# get the answer\n",
    "answer = get_answer(question, context)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the documents folder\n",
    "import os\n",
    "\n",
    "for file in os.listdir('Documents'):\n",
    "    os.remove('Documents/' + file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class DocumentReader:\n",
    "    def __init__(self, pretrained_model_name_or_path='bert-large-uncased'):\n",
    "        self.READER_PATH = pretrained_model_name_or_path\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.READER_PATH)\n",
    "        self.model = AutoModelForQuestionAnswering.from_pretrained(self.READER_PATH)\n",
    "        self.max_len = self.model.config.max_position_embeddings\n",
    "        self.chunked = False\n",
    "\n",
    "    def tokenize(self, question, text):\n",
    "        self.inputs = self.tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "        self.input_ids = self.inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "        if len(self.input_ids) > self.max_len:\n",
    "            self.inputs = self.chunkify()\n",
    "            self.chunked = True\n",
    "\n",
    "    def chunkify(self):\n",
    "        \"\"\" \n",
    "        Break up a long article into chunks that fit within the max token\n",
    "        requirement for that Transformer model. \n",
    "\n",
    "        Calls to BERT / RoBERTa / ALBERT require the following format:\n",
    "        [CLS] question tokens [SEP] context tokens [SEP].\n",
    "        \"\"\"\n",
    "\n",
    "        # create question mask based on token_type_ids\n",
    "        # value is 0 for question tokens, 1 for context tokens\n",
    "        qmask = self.inputs['token_type_ids'].lt(1)\n",
    "        qt = torch.masked_select(self.inputs['input_ids'], qmask)\n",
    "        chunk_size = self.max_len - qt.size()[0] - 1 # the \"-1\" accounts for\n",
    "        # having to add an ending [SEP] token to the end\n",
    "\n",
    "        # create a dict of dicts; each sub-dict mimics the structure of pre-chunked model input\n",
    "        chunked_input = OrderedDict()\n",
    "        for k,v in self.inputs.items():\n",
    "            q = torch.masked_select(v, qmask)\n",
    "            c = torch.masked_select(v, ~qmask)\n",
    "            chunks = torch.split(c, chunk_size)\n",
    "            \n",
    "            for i, chunk in enumerate(chunks):\n",
    "                if i not in chunked_input:\n",
    "                    chunked_input[i] = {}\n",
    "\n",
    "                thing = torch.cat((q, chunk))\n",
    "                if i != len(chunks)-1:\n",
    "                    if k == 'input_ids':\n",
    "                        thing = torch.cat((thing, torch.tensor([102])))\n",
    "                    else:\n",
    "                        thing = torch.cat((thing, torch.tensor([1])))\n",
    "\n",
    "                chunked_input[i][k] = torch.unsqueeze(thing, dim=0)\n",
    "        return chunked_input\n",
    "\n",
    "    def get_answer(self):\n",
    "        if self.chunked:\n",
    "            answer = ''\n",
    "            for k, chunk in self.inputs.items():\n",
    "                answer_start_scores, answer_end_scores = self.model(**chunk)\n",
    "\n",
    "                answer_start = torch.argmax(answer_start_scores)\n",
    "                answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "                ans = self.convert_ids_to_string(chunk['input_ids'][0][answer_start:answer_end])\n",
    "                if ans != '[CLS]':\n",
    "                    answer += ans + \" / \"\n",
    "            return answer\n",
    "        else:\n",
    "            answer_start_scores, answer_end_scores = self.model(**self.inputs)\n",
    "\n",
    "            answer_start = torch.argmax(answer_start_scores)  # get the most likely beginning of answer with the argmax of the score\n",
    "            answer_end = torch.argmax(answer_end_scores) + 1  # get the most likely end of answer with the argmax of the score\n",
    "        \n",
    "            return self.convert_ids_to_string(self.inputs['input_ids'][0][\n",
    "                                              answer_start:answer_end])\n",
    "\n",
    "    def convert_ids_to_string(self, input_ids):\n",
    "        return self.tokenizer.convert_tokens_to_string(self.tokenizer.convert_ids_to_tokens(input_ids))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
