{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devgoyal/anaconda3/envs/research/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DocumentEmbedder:\n",
    "    def __init__(self, model_name='bert-large-uncased'):\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = BertModel.from_pretrained(model_name)\n",
    "    def embed_doc(self, documents):\n",
    "        # Initialize an empty list to store the document embeddings\n",
    "        document_embeddings = []\n",
    "\n",
    "        # Loop through each document and embed it\n",
    "        for doc in documents:\n",
    "            # Tokenize the document and convert it to tensors\n",
    "            inputs = self.tokenizer(doc, return_tensors='pt', truncation=True, padding=True)\n",
    "            \n",
    "            # Forward pass through the BERT model\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                # Extract the last layer embeddings for each token\n",
    "                embeddings = outputs.last_hidden_state\n",
    "            \n",
    "            # Calculate the mean of token embeddings to get document-level embedding\n",
    "            doc_embedding = torch.mean(embeddings, dim=1).squeeze().numpy()\n",
    "            document_embeddings.append(doc_embedding)\n",
    "        return document_embeddings\n",
    "    \n",
    "    def encode_question(self, question, max_length=128):\n",
    "        tokens = self.tokenizer.encode(question, add_special_tokens=True, max_length=max_length, truncation=True)\n",
    "\n",
    "        # Convert tokens to tensors\n",
    "        input_ids = torch.tensor([tokens])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids)\n",
    "            question_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        # Convert the PyTorch tensor to a NumPy array\n",
    "        question_embedding_np = question_embedding.numpy()\n",
    "\n",
    "        return question_embedding_np\n",
    "\n",
    "bert = DocumentEmbedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from serpapi import GoogleSearch\n",
    "\n",
    "def get_query(query):\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"engine\": \"google\",\n",
    "        \"api_key\": \"d5283d8a6c6640c36e5228ae57e8baa9170859f8b5fa73e3c941cdb51afa9e0f\"\n",
    "    }\n",
    "\n",
    "    search = GoogleSearch(params)\n",
    "    results = search.get_dict()\n",
    "    organic_results = results['organic_results']\n",
    "    return organic_results\n",
    "\n",
    "Query = \"Professor Kevin Chang uiuc\"\n",
    "organic_results = get_query(Query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trafilatura as tr\n",
    "import pandas as pd\n",
    "import wikipedia as wiki\n",
    "\n",
    "def parse_results(link):\n",
    "    downloaded = tr.fetch_url(link)\n",
    "    text = tr.extract(downloaded, include_formatting=True)\n",
    "    if text == None:\n",
    "        return ''\n",
    "    return text\n",
    "\n",
    "def get_scholar_results(author_id):\n",
    "    params = {\n",
    "        'engine': 'google_scholar_author',\n",
    "        'author_id': author_id,\n",
    "        'api_key' : 'd5283d8a6c6640c36e5228ae57e8baa9170859f8b5fa73e3c941cdb51afa9e0f'\n",
    "    }\n",
    "    search = GoogleSearch(params)\n",
    "    results = search.get_dict()\n",
    "    return results\n",
    "\n",
    "dataset = {'titles': [], 'text': []}\n",
    "\n",
    "named_entity = \"\"\n",
    "for result in organic_results:\n",
    "    if 'wikipedia.org' in result['link']:\n",
    "        with open(\"Documents/wiki.txt\", 'w') as f:\n",
    "            wiki_results = wiki.search(result['title'])\n",
    "            page = wiki.page(wiki_results[0], auto_suggest=False)\n",
    "            f.write(page.content)\n",
    "        dataset['titles'].append(\"wikiresult\")\n",
    "        dataset['text'].append(page.content)\n",
    "            \n",
    "    # if 'scholar.google.com' in result['link']:\n",
    "    #     # get user id from link\n",
    "    #     user_id = result['link'].split('user=')[1].split('&')[0]\n",
    "    #     # get scholar results\n",
    "    #     scholar_results = get_scholar_results(user_id)\n",
    "    #     print(scholar_results[\"author\"])\n",
    "    else:\n",
    "        text = parse_results(result['link'])\n",
    "        if named_entity.lower() in text.lower():\n",
    "            with open(\"Documents/\" + result['title'] + '.txt', 'w') as f:\n",
    "                f.write(parse_results(result['link']))\n",
    "            dataset['titles'].append(result['title'])\n",
    "            dataset['text'].append(parse_results(result['link']))\n",
    "\n",
    "df = pd.DataFrame(dataset)\n",
    "# df.to_csv('Dataset.csv', index=False, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the text into passages\n",
    "\n",
    "def split_text(text: str, n=100, character=\" \"):\n",
    "    \"\"\"Split the text every ``n``-th occurrence of ``character``\"\"\"\n",
    "    text = text.split(character)\n",
    "    return [character.join(text[i : i + n]).strip() for i in range(0, len(text), n)]\n",
    "\n",
    "\n",
    "def split_documents(documents: dict) -> dict:\n",
    "    \"\"\"Split documents into passages\"\"\"\n",
    "    titles, texts = [], []\n",
    "    for title, text in zip(documents[\"titles\"], documents[\"text\"]):\n",
    "        if text is not None:\n",
    "            for passage in split_text(text):\n",
    "                titles.append(title if title is not None else \"\")\n",
    "                texts.append(passage)\n",
    "    return {\"title\": titles, \"text\": texts}\n",
    "\n",
    "documents = df.to_dict('list')\n",
    "documents = split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "documents[\"embedding\"] = bert.embed_doc(documents[\"text\"])\n",
    "print(len(documents[\"embedding\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38, 3)\n"
     ]
    }
   ],
   "source": [
    "new_df_embed = pd.DataFrame(documents)\n",
    "print(new_df_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Kevin Chenchuan Chang - Computer Science | UIUC': 0, 'Kevin C.C. Chang - Professor - University of Illinois at ...': 1, 'Kevin Chen-Chuan Chang': 2, 'Kevin Chang at University Of Illinois at Urbana - Champaign': 3, 'Kevin Chang - Professor @ University of Illinois Urbana ...': 4, 'Kevin Chen-Chuan Chang | IEEE Xplore Author Details': 5}\n"
     ]
    }
   ],
   "source": [
    "label_to_id = {label: i for i, label in enumerate(new_df_embed['title'].unique())}\n",
    "print(label_to_id)\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kevin Chenchuan Chang - Computer Science | UIUC</td>\n",
       "      <td># Kevin Chenchuan Chang\\n## For More Informati...</td>\n",
       "      <td>[-0.3566605, -0.19457227, -0.0086028995, -0.19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kevin Chenchuan Chang - Computer Science | UIUC</td>\n",
       "      <td>2002, NCSA Faculty Fellow Award in 2003, IBM F...</td>\n",
       "      <td>[-0.3238592, -0.20354173, -0.20277858, -0.1660...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kevin Chenchuan Chang - Computer Science | UIUC</td>\n",
       "      <td>National Institutes of Health.\\n## Professiona...</td>\n",
       "      <td>[-0.13917094, -0.09737851, -0.010339749, -0.10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kevin Chenchuan Chang - Computer Science | UIUC</td>\n",
       "      <td>Award Committee, ACM International Conference ...</td>\n",
       "      <td>[-0.22010683, -0.1606884, -0.054008003, -0.221...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kevin Chenchuan Chang - Computer Science | UIUC</td>\n",
       "      <td>Retrieval and Integration (WIRI 2006) at ICDE,...</td>\n",
       "      <td>[-0.3644765, -0.22623837, -0.014934646, -0.180...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Kevin Chenchuan Chang - Computer Science | UIUC</td>\n",
       "      <td>*machine learning*. As our objectives, we aim ...</td>\n",
       "      <td>[-0.17899835, -0.3444038, -0.21057726, -0.1656...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Kevin Chenchuan Chang - Computer Science | UIUC</td>\n",
       "      <td>Conference Proceedings\\n- Are Large Pre-Traine...</td>\n",
       "      <td>[-0.3591724, -0.22857207, -0.026724678, -0.151...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Kevin Chenchuan Chang - Computer Science | UIUC</td>\n",
       "      <td>in Natural Language Processing, EMNLP 2022, Ab...</td>\n",
       "      <td>[-0.35645, -0.18544164, 0.05279978, -0.0866148...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Kevin Chenchuan Chang - Computer Science | UIUC</td>\n",
       "      <td>2022, 2022.\\n- Open Relation Modeling: Learnin...</td>\n",
       "      <td>[-0.27726692, -0.20850885, -0.040968973, -0.15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Kevin Chenchuan Chang - Computer Science | UIUC</td>\n",
       "      <td>Association for Computational Linguistics and ...</td>\n",
       "      <td>[-0.28778446, -0.16929694, 0.01353539, -0.1753...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Kevin Chenchuan Chang - Computer Science | UIUC</td>\n",
       "      <td>Chen-Chuan Chang, Yu Lei, Bo Yang. In 8th Inte...</td>\n",
       "      <td>[-0.33149898, -0.19030096, 0.029194439, -0.090...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Kevin Chenchuan Chang - Computer Science | UIUC</td>\n",
       "      <td>Han, Hongtai Cao. In CIKM '20: The 29th ACM In...</td>\n",
       "      <td>[-0.2875063, -0.26716685, 0.041113004, -0.1350...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Kevin Chenchuan Chang - Computer Science | UIUC</td>\n",
       "      <td>Systems 33: Annual Conference on Neural Inform...</td>\n",
       "      <td>[-0.17997074, -0.10591066, -0.08192504, -0.056...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Kevin Chenchuan Chang - Computer Science | UIUC</td>\n",
       "      <td>Fellow Award, 2008.\\n- IBM Faculty Award, 2005...</td>\n",
       "      <td>[-0.1573882, -0.15812866, -0.12225146, -0.0767...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Kevin C.C. Chang - Professor - University of I...</td>\n",
       "      <td></td>\n",
       "      <td>[-0.39606196, 0.17737082, 0.17496899, 0.567650...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Kevin Chen-Chuan Chang</td>\n",
       "      <td>|A comprehensive survey of graph embedding: Pr...</td>\n",
       "      <td>[-0.19116656, -0.07645921, 0.040273156, -0.121...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Kevin Chen-Chuan Chang</td>\n",
       "      <td>on data engineering, 896-905, 2006\\n|601||2006...</td>\n",
       "      <td>[-0.27579433, -0.12117914, 0.037134476, -0.138...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Kevin Chen-Chuan Chang</td>\n",
       "      <td>CCK Chang, H Garcia-Molina, A Paepcke\\nProceed...</td>\n",
       "      <td>[-0.27447596, -0.14107035, 0.12658146, -0.1714...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Kevin Chen-Chuan Chang</td>\n",
       "      <td>optimization for relational top-k queries|\\nC ...</td>\n",
       "      <td>[-0.2931689, -0.11438438, 0.10128197, -0.11910...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Kevin Chen-Chuan Chang</td>\n",
       "      <td>MetaQuerier over Databases on the Web.|\\nKCC C...</td>\n",
       "      <td>[-0.32617506, -0.122041345, -0.024121288, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Kevin Chang at University Of Illinois at Urban...</td>\n",
       "      <td>Professors\\ncancel\\nat\\nLog In\\nSign Up\\n3.5\\n...</td>\n",
       "      <td>[-0.19496438, -0.26092678, -0.19157709, -0.207...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Kevin Chang at University Of Illinois at Urban...</td>\n",
       "      <td>was great at explaining the concepts and revie...</td>\n",
       "      <td>[-0.21396656, -0.21287933, -0.23586696, -0.215...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Kevin Chang at University Of Illinois at Urban...</td>\n",
       "      <td>\"what does assignment X mean\" he will be \"Didn...</td>\n",
       "      <td>[-0.2689068, -0.24825832, -0.1673699, -0.19575...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Kevin Chang at University Of Illinois at Urban...</td>\n",
       "      <td>Grading was fair. He actually listened to the ...</td>\n",
       "      <td>[-0.23027602, -0.32227117, -0.08640252, -0.249...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Kevin Chang at University Of Illinois at Urban...</td>\n",
       "      <td>of tricky questions which don't test your unde...</td>\n",
       "      <td>[-0.25595507, -0.29125163, -0.20736395, -0.249...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Kevin Chang at University Of Illinois at Urban...</td>\n",
       "      <td>Take Again:\\nNo\\nGrade:\\nRather not say\\nTextb...</td>\n",
       "      <td>[-0.25674278, -0.25102815, -0.21527067, -0.160...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Kevin Chang at University Of Illinois at Urban...</td>\n",
       "      <td>Again:\\nYes\\nTextbook:\\nNo\\nOnline Class:\\nYes...</td>\n",
       "      <td>[-0.17478271, -0.34348854, -0.2088942, -0.2396...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Kevin Chang at University Of Illinois at Urban...</td>\n",
       "      <td>intriguing and inspiring. Much reading. Heavy ...</td>\n",
       "      <td>[-0.25157097, -0.2923226, -0.19813024, -0.2551...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Kevin Chang at University Of Illinois at Urban...</td>\n",
       "      <td>Homeworks took a lot of time, but you could ge...</td>\n",
       "      <td>[-0.19110009, -0.20092642, -0.19625509, -0.213...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Kevin Chang at University Of Illinois at Urban...</td>\n",
       "      <td>23rd, 2015\\nQuality\\n5.0\\nDifficulty\\n1.0\\nCS4...</td>\n",
       "      <td>[-0.2563845, -0.11525103, -0.30456007, -0.4038...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Kevin Chen-Chuan Chang</td>\n",
       "      <td></td>\n",
       "      <td>[-0.39606196, 0.17737082, 0.17496899, 0.567650...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Kevin Chang - Professor @ University of Illino...</td>\n",
       "      <td>Primary Job Title Professor Primary Organizati...</td>\n",
       "      <td>[-0.34187528, -0.20757045, -0.1575469, -0.2667...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Kevin Chang - Professor @ University of Illino...</td>\n",
       "      <td>and a Ph.D. in Electrical Engineering from Sta...</td>\n",
       "      <td>[-0.29153213, -0.18828084, -0.07174013, -0.176...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Kevin Chen-Chuan Chang | IEEE Xplore Author De...</td>\n",
       "      <td>A not-for-profit organization, IEEE is the wor...</td>\n",
       "      <td>[-0.381818, -0.32932803, -0.46700877, -0.23015...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Kevin Chen-Chuan Chang</td>\n",
       "      <td>基本信息\\n浏览量：1173\\n职业迁徙\\n个人简介\\nKevin C. Chang is ...</td>\n",
       "      <td>[-0.40544108, -0.17916735, 0.06823222, -0.2616...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Kevin Chen-Chuan Chang</td>\n",
       "      <td>the Incomplete List of Excellent Teachers at U...</td>\n",
       "      <td>[-0.2502442, -0.3123347, -0.20795754, -0.18346...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Kevin Chen-Chuan Chang</td>\n",
       "      <td>the world's information. Therefore, our resear...</td>\n",
       "      <td>[-0.24435161, -0.34754607, -0.17799598, -0.122...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0     Kevin Chenchuan Chang - Computer Science | UIUC   \n",
       "1     Kevin Chenchuan Chang - Computer Science | UIUC   \n",
       "2     Kevin Chenchuan Chang - Computer Science | UIUC   \n",
       "3     Kevin Chenchuan Chang - Computer Science | UIUC   \n",
       "4     Kevin Chenchuan Chang - Computer Science | UIUC   \n",
       "5     Kevin Chenchuan Chang - Computer Science | UIUC   \n",
       "6     Kevin Chenchuan Chang - Computer Science | UIUC   \n",
       "7     Kevin Chenchuan Chang - Computer Science | UIUC   \n",
       "8     Kevin Chenchuan Chang - Computer Science | UIUC   \n",
       "9     Kevin Chenchuan Chang - Computer Science | UIUC   \n",
       "10    Kevin Chenchuan Chang - Computer Science | UIUC   \n",
       "11    Kevin Chenchuan Chang - Computer Science | UIUC   \n",
       "12    Kevin Chenchuan Chang - Computer Science | UIUC   \n",
       "13    Kevin Chenchuan Chang - Computer Science | UIUC   \n",
       "14  Kevin C.C. Chang - Professor - University of I...   \n",
       "15                             Kevin Chen-Chuan Chang   \n",
       "16                             Kevin Chen-Chuan Chang   \n",
       "17                             Kevin Chen-Chuan Chang   \n",
       "18                             Kevin Chen-Chuan Chang   \n",
       "19                             Kevin Chen-Chuan Chang   \n",
       "20  Kevin Chang at University Of Illinois at Urban...   \n",
       "21  Kevin Chang at University Of Illinois at Urban...   \n",
       "22  Kevin Chang at University Of Illinois at Urban...   \n",
       "23  Kevin Chang at University Of Illinois at Urban...   \n",
       "24  Kevin Chang at University Of Illinois at Urban...   \n",
       "25  Kevin Chang at University Of Illinois at Urban...   \n",
       "26  Kevin Chang at University Of Illinois at Urban...   \n",
       "27  Kevin Chang at University Of Illinois at Urban...   \n",
       "28  Kevin Chang at University Of Illinois at Urban...   \n",
       "29  Kevin Chang at University Of Illinois at Urban...   \n",
       "30                             Kevin Chen-Chuan Chang   \n",
       "31  Kevin Chang - Professor @ University of Illino...   \n",
       "32  Kevin Chang - Professor @ University of Illino...   \n",
       "33  Kevin Chen-Chuan Chang | IEEE Xplore Author De...   \n",
       "34                             Kevin Chen-Chuan Chang   \n",
       "35                             Kevin Chen-Chuan Chang   \n",
       "36                             Kevin Chen-Chuan Chang   \n",
       "\n",
       "                                                 text  \\\n",
       "0   # Kevin Chenchuan Chang\\n## For More Informati...   \n",
       "1   2002, NCSA Faculty Fellow Award in 2003, IBM F...   \n",
       "2   National Institutes of Health.\\n## Professiona...   \n",
       "3   Award Committee, ACM International Conference ...   \n",
       "4   Retrieval and Integration (WIRI 2006) at ICDE,...   \n",
       "5   *machine learning*. As our objectives, we aim ...   \n",
       "6   Conference Proceedings\\n- Are Large Pre-Traine...   \n",
       "7   in Natural Language Processing, EMNLP 2022, Ab...   \n",
       "8   2022, 2022.\\n- Open Relation Modeling: Learnin...   \n",
       "9   Association for Computational Linguistics and ...   \n",
       "10  Chen-Chuan Chang, Yu Lei, Bo Yang. In 8th Inte...   \n",
       "11  Han, Hongtai Cao. In CIKM '20: The 29th ACM In...   \n",
       "12  Systems 33: Annual Conference on Neural Inform...   \n",
       "13  Fellow Award, 2008.\\n- IBM Faculty Award, 2005...   \n",
       "14                                                      \n",
       "15  |A comprehensive survey of graph embedding: Pr...   \n",
       "16  on data engineering, 896-905, 2006\\n|601||2006...   \n",
       "17  CCK Chang, H Garcia-Molina, A Paepcke\\nProceed...   \n",
       "18  optimization for relational top-k queries|\\nC ...   \n",
       "19  MetaQuerier over Databases on the Web.|\\nKCC C...   \n",
       "20  Professors\\ncancel\\nat\\nLog In\\nSign Up\\n3.5\\n...   \n",
       "21  was great at explaining the concepts and revie...   \n",
       "22  \"what does assignment X mean\" he will be \"Didn...   \n",
       "23  Grading was fair. He actually listened to the ...   \n",
       "24  of tricky questions which don't test your unde...   \n",
       "25  Take Again:\\nNo\\nGrade:\\nRather not say\\nTextb...   \n",
       "26  Again:\\nYes\\nTextbook:\\nNo\\nOnline Class:\\nYes...   \n",
       "27  intriguing and inspiring. Much reading. Heavy ...   \n",
       "28  Homeworks took a lot of time, but you could ge...   \n",
       "29  23rd, 2015\\nQuality\\n5.0\\nDifficulty\\n1.0\\nCS4...   \n",
       "30                                                      \n",
       "31  Primary Job Title Professor Primary Organizati...   \n",
       "32  and a Ph.D. in Electrical Engineering from Sta...   \n",
       "33  A not-for-profit organization, IEEE is the wor...   \n",
       "34  基本信息\\n浏览量：1173\\n职业迁徙\\n个人简介\\nKevin C. Chang is ...   \n",
       "35  the Incomplete List of Excellent Teachers at U...   \n",
       "36  the world's information. Therefore, our resear...   \n",
       "\n",
       "                                            embedding  \n",
       "0   [-0.3566605, -0.19457227, -0.0086028995, -0.19...  \n",
       "1   [-0.3238592, -0.20354173, -0.20277858, -0.1660...  \n",
       "2   [-0.13917094, -0.09737851, -0.010339749, -0.10...  \n",
       "3   [-0.22010683, -0.1606884, -0.054008003, -0.221...  \n",
       "4   [-0.3644765, -0.22623837, -0.014934646, -0.180...  \n",
       "5   [-0.17899835, -0.3444038, -0.21057726, -0.1656...  \n",
       "6   [-0.3591724, -0.22857207, -0.026724678, -0.151...  \n",
       "7   [-0.35645, -0.18544164, 0.05279978, -0.0866148...  \n",
       "8   [-0.27726692, -0.20850885, -0.040968973, -0.15...  \n",
       "9   [-0.28778446, -0.16929694, 0.01353539, -0.1753...  \n",
       "10  [-0.33149898, -0.19030096, 0.029194439, -0.090...  \n",
       "11  [-0.2875063, -0.26716685, 0.041113004, -0.1350...  \n",
       "12  [-0.17997074, -0.10591066, -0.08192504, -0.056...  \n",
       "13  [-0.1573882, -0.15812866, -0.12225146, -0.0767...  \n",
       "14  [-0.39606196, 0.17737082, 0.17496899, 0.567650...  \n",
       "15  [-0.19116656, -0.07645921, 0.040273156, -0.121...  \n",
       "16  [-0.27579433, -0.12117914, 0.037134476, -0.138...  \n",
       "17  [-0.27447596, -0.14107035, 0.12658146, -0.1714...  \n",
       "18  [-0.2931689, -0.11438438, 0.10128197, -0.11910...  \n",
       "19  [-0.32617506, -0.122041345, -0.024121288, -0.0...  \n",
       "20  [-0.19496438, -0.26092678, -0.19157709, -0.207...  \n",
       "21  [-0.21396656, -0.21287933, -0.23586696, -0.215...  \n",
       "22  [-0.2689068, -0.24825832, -0.1673699, -0.19575...  \n",
       "23  [-0.23027602, -0.32227117, -0.08640252, -0.249...  \n",
       "24  [-0.25595507, -0.29125163, -0.20736395, -0.249...  \n",
       "25  [-0.25674278, -0.25102815, -0.21527067, -0.160...  \n",
       "26  [-0.17478271, -0.34348854, -0.2088942, -0.2396...  \n",
       "27  [-0.25157097, -0.2923226, -0.19813024, -0.2551...  \n",
       "28  [-0.19110009, -0.20092642, -0.19625509, -0.213...  \n",
       "29  [-0.2563845, -0.11525103, -0.30456007, -0.4038...  \n",
       "30  [-0.39606196, 0.17737082, 0.17496899, 0.567650...  \n",
       "31  [-0.34187528, -0.20757045, -0.1575469, -0.2667...  \n",
       "32  [-0.29153213, -0.18828084, -0.07174013, -0.176...  \n",
       "33  [-0.381818, -0.32932803, -0.46700877, -0.23015...  \n",
       "34  [-0.40544108, -0.17916735, 0.06823222, -0.2616...  \n",
       "35  [-0.2502442, -0.3123347, -0.20795754, -0.18346...  \n",
       "36  [-0.24435161, -0.34754607, -0.17799598, -0.122...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df_embed.head(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_features shape =  (38, 1024)\n",
      "train_labels shape =  (38,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "train_features = np.array(new_df_embed['embedding'].tolist())\n",
    "train_labels = np.array([label_to_id[i] for i in new_df_embed['title'].tolist()])\n",
    "print(\"train_features shape = \", train_features.shape)\n",
    "print(\"train_labels shape = \", train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(metric=&#x27;cosine&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(metric=&#x27;cosine&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier(metric='cosine')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create a KNN classifier with cosine similarity as the metric\n",
    "knn = KNeighborsClassifier(metric='cosine')\n",
    "\n",
    "# Fit the KNN model with document embeddings\n",
    "knn.fit(train_features, train_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\"List the recent courses taught?\", \"Where did Professor get his PhD?\", \"Where did professor study from?\", \"What does the professor teach?\", \"What is the professor's research interest?\"]\n",
    "question_embedding = [bert.encode_question(question)[0] for question in questions]\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kevin Chenchuan Chang - Computer Science | UIUC\n",
      "Kevin Chenchuan Chang - Computer Science | UIUC\n",
      "Kevin Chenchuan Chang - Computer Science | UIUC\n",
      "Kevin Chenchuan Chang - Computer Science | UIUC\n",
      "Kevin Chenchuan Chang - Computer Science | UIUC\n"
     ]
    }
   ],
   "source": [
    "# get the top k most relevant documents\n",
    "k = 1\n",
    "top_k = knn.predict(question_embedding)\n",
    "\n",
    "for i in top_k:\n",
    "    print(id_to_label[i])\n",
    "\n",
    "# concatenate the text from the top k documents\n",
    "context = df.iloc[top_k]['text'].str.cat(sep=' ')\n",
    "# print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Kevin Chenchuan Chang',\n",
       " 'For More Information',\n",
       " 'Education\\n- Ph.D. Electrial Engineering, Stanford University, 2001',\n",
       " 'Biography\\nKevin Chen-Chuan Chang is a Professor in Computer Science, University of Illinois at Urbana-Champaign. He received a BS from National Taiwan University and PhD from Stanford University in Electrical Engineering. His research addresses large-scale information access and knowledge acquisition, for search, mining, and integration across structured and unstructured big data, with current focuses on Web search/mining and social media analytics. He received ICDE 10-Year Test of Time Award in 2022 and Best Paper Selection/Awards in VLDB 2000 and 2013 and ASONAM 2019, NSF CAREER Award in 2002, NCSA Faculty Fellow Award in 2003, IBM Faculty Awards in 2004 and 2005, Academy for Entrepreneurial Leadership Faculty Fellow Award in 2008, and the Incomplete List of Excellent Teachers at University of Illinois in 2001, 2004, 2005, 2006, 2010, 2011, 2019, 2022, 2023. He is passionate to bring research results to the real world and, with his students, co-founded Cazoodle, a startup from the University of Illinois, and developed GrantForward.com funding discovery and dissemination service, a vertical search engine integrating 20,000 sources, subscribed by 200+ institutions including Harvard, Stanford, Yale, Cornell University, University of California, CMU, Mayo Clinic, and National Institutes of Health.',\n",
       " 'Professional Highlights\\n- PC Members, SIGMOD, VLDB, ICDE, KDD, EDBT, ICDM, WWW, ASONAM, SIGIR, WSDM, CIKM, AAAI, Recent years.\\n- Area Chair, NeurIPS 2023, 2023.\\n- Associate Editor, Proceedings of the VLDB Endowment (PVLDB), 2014 - 2015.\\n- Area Editor, Encyclopedia of Database Systems, 2014 - 2016.\\n- Workshop Co-chair, 31st IEEE International Conference on Data Engineering (ICDE 2015), 2014 - 2015.\\n- Associate Editor, IEEE Transactions on Knowledge and Data Engineering (TKDE), 2013 - 2017.\\n- Workshop Co-chair, 22rd International World Wide Web Conference (WWW 2014), 2013 - 2014.\\n- PC Co-chair, Track “Bringing Unstructured and Structured Data”, 2012 - 2013.\\n- Best Paper Award Committee, ACM International Conference on Web Search and Data Mining, 2012.\\n- Senior PC, ACM International Conference on Web Search and Data Mining, 2011.\\n- Co-chair, Demonstration Track, ICDE 2011, 2011.\\n- Senior PC, ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2010.\\n- Area Editor, Encyclopedia of Database Systems, 2007 - 2009.\\n- Workshop Chair, APWeb 2007, 2007.\\n- Steering Committee, International Workshop on Information Integration on the Web (IIWeb 2007) at AAAI, 2007.\\n- Workshop Chair, ACM SIGMOD 2006 Conference, 2006.\\n- Co-chair, International Workshop on Information Integration on the Web (IIWeb 2006) at WWW, 2006.\\n- Co-chair, International Workshop on Challenges in Web Information Retrieval and Integration (WIRI 2006) at ICDE, 2006.\\n- Guest Editor, SIGKDD Explorations 6(2) Special Issue on Web Content Mining, 2004.\\n- Chair, NSF DIMACS Center Tutorial/Summer School on Social Choice and Computer Science, 2004.',\n",
       " 'Research Statement\\nI lead the FORWARD Data Lab group, which is part of the larger Data and Information Systems Laboratories, at the CS department of UIUC. Our research overall aims at bridging\\n*structured* and *unstructured data*— to bring structured/semantic-rich access to the myriad and massive unstructured data which accounts for most of the world’s information. Therefore, our research spans *natural language processing*, *data mining*, *data management/databases*, *information retrieval*, and *machine learning*. As our objectives, we aim at developing novel systems, principled algorithms, and formal theories that ultimately deliver real-world applications. As our approaches, we seek to be inspired by and learn from the data we are tackling-- i.e., we believe the key to tame big data is to learn the wisdom hidden in the large scale of the data.',\n",
       " 'Research Interests\\n- Natural language processing, data mining, data management, and information retrieval with machine learning techniques, with emphasis on the application areas of Web and social media-based knowledge acquisition and organization across structured and unstructured data.',\n",
       " 'Research Areas',\n",
       " \"Articles in Conference Proceedings\\n- Are Large Pre-Trained Language Models Leaking Your Personal Information?. Jie Huang, Hanyin Shao, Kevin Chen-Chuan Chang. In Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, 2022.\\n- Coordinated Topic Modeling. Pritom Saha Akash, Jie Huang, Kevin Chen-Chuan Chang. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, 2022.\\n- DEER: Descriptive Knowledge Graph for Explaining Entity Relationships. Jie Huang, Kerui Zhu, Kevin Chen-Chuan Chang, Jinjun Xiong, Wen-Mei Hwu. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, 2022.\\n- Understanding Jargon: Combining Extraction and Generation for Definition Modeling. Jie Huang, Hanyin Shao, Kevin Chen-Chuan Chang, Jinjun Xiong, Wen-Mei Hwu. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, 2022.\\n- Unified and Incremental SimRank: Index-Free Approximation With Scheduled Principle (Extended Abstract). Fanwei Zhu, Yuan Fang, Kai Zhang, Kevin Chen-Chuan Chang, Hongtai Cao, Zhen Jiang, Minghui Wu. In 38th IEEE International Conference on Data Engineering, ICDE 2022, Kuala Lumpur, Malaysia, May 9-12, 2022, 2022.\\n- Open Relation Modeling: Learning to Define Relations Between Entities. Jie Huang, Kevin Chen-Chuan Chang, Jinjun Xiong, Wen-Mei Hwu. In Findings of the Association for Computational Linguistics: ACL 2022, Dublin, Ireland, May 22-27, 2022, 2022.\\n- Domain Representative Keywords Selection: A Probabilistic Approach. Pritom Saha Akash, Jie Huang, Kevin Chen-Chuan Chang, Yunyao Li, Lucian Popa, ChengXiang Zhai. In Findings of the Association for Computational Linguistics: ACL 2022, Dublin, Ireland, May 22-27, 2022, 2022.\\n- Measuring Fine-Grained Domain Relevance of Terms: A Hierarchical Core-Fringe Approach. Jie Huang, Kevin Chang, Jinjun Xiong, Wen-Mei Hwu. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, 2021.\\n- On Analyzing Graphs With Motif-Paths. Xiaodong Li, Reynold Cheng, Kevin Chen-Chuan Chang, Caihua Shan, Chenhao Ma, Hongtai Cao. In VLDB 2021, 2021.\\n- MC-Explorer: Analyzing and Visualizing Motif-Cliques on Large Networks. Boxuan Li, Reynold Cheng, Jiafeng Hu, Yixiang Fang, Min Ou, Ruibang Luo, Kevin Chen-Chuan Chang, Xuemin Lin. In 36th IEEE International Conference on Data Engineering, ICDE 2020, Dallas, TX, USA, April 20-24, 2020, 2020.\\n- Geom-GCN: Geometric Graph Convolutional Networks. Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, Bo Yang. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020.\\n- ROSE: Role-Based Signed Network Embedding. Amin Javari, Tyler Derr, Pouya Esmailian, Jiliang Tang, Kevin Chen-Chuan Chang. In WWW '20: The Web Conference 2020, Taipei, Taiwan, April 20-24, 2020, 2020.\\n- Weakly Supervised Attention for Hashtag Recommendation Using Graph Data. Amin Javari, Zhankui He, Zijie Huang, Jeetu Raj, Kevin Chen-Chuan Chang. In WWW '20: The Web Conference 2020, Taipei, Taiwan, April 20-24, 2020, 2020.\\n- M-Cypher: A GQL Framework Supporting Motifs. Xiaodong Li, Reynold Cheng, Matin Najafi, Kevin Chen-Chuan Chang, Xiaolin Han, Hongtai Cao. In CIKM '20: The 29th ACM International Conference on Information and Knowledge Management, Virtual Event, Ireland, October 19-23, 2020, 2020.\\n- GraphEBM: Energy-based Graph Construction for Semi-Supervised Learning. Zhijie Chen, Hongtai Cao,, Kevin Chen-Chuan Chang. In ICDM 2020, 2020.\\n- Exploring Semantic Capacity of Terms. Jie Huang, Zilong Wang, Kevin Chang, Wen-Mei Hwu, Jinjun Xiong. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, 2020.\\n- Curvature Regularization to Prevent Distortion in Graph Embedding. Hongbin Pei, Bingzhe Wei, Kevin Chang, Chunxu Zhang, Bo Yang. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\",\n",
       " 'Teaching Honors\\n- UIUC List of Teachers Ranked as Excellent by Their Students, Fall 2001, Spring 2004, Fall 2005, Spring 2006, Fall 2010, Fall 2011, Fall 2019, Spring 2022, Spring 2023.',\n",
       " 'Research Honors\\n- ICDE Influential Paper (10-Year Test of Time) Award, IEEE Computer Society, 2022.\\n- Best Demo Award,IEEE International Conference on Data Engineering (ICDE) 2019.\\n- Best Paper Award, International Conference on Advances in Social Networks Analysis and Mining (ASONAM), 2019.\\n- Best-Papers Selections of Very Large Data Bases (VLDB) 2013.\\n- Academy of Entrepreneurial Leadership Faculty Fellow Award, 2008.\\n- IBM Faculty Award, 2005.\\n- IBM Faculty Award, 2004.\\n- NCSA (National Center for Supercomputing Applications) Faculty Fellows Award, 2003.\\n- National Science Foundation CAREER Award 2002.\\n- Best-Papers Selections of Very Large Data Bases (VLDB) 2000.\\n- Philips FMA Fellowships, 1996-1998.',\n",
       " 'Recent Courses Taught\\n- CS 411 - Database Systems\\n- CS 412 CSP - Introduction to Data Mining\\n- CS 511 - Advanced Data Management\\n- CS 598 KCC (CS 598 KCO) - Listening to Social Universe',\n",
       " 'Kevin Chenchuan Chang',\n",
       " 'For More Information',\n",
       " 'Education\\n- Ph.D. Electrial Engineering, Stanford University, 2001',\n",
       " 'Biography\\nKevin Chen-Chuan Chang is a Professor in Computer Science, University of Illinois at Urbana-Champaign. He received a BS from National Taiwan University and PhD from Stanford University in Electrical Engineering. His research addresses large-scale information access and knowledge acquisition, for search, mining, and integration across structured and unstructured big data, with current focuses on Web search/mining and social media analytics. He received ICDE 10-Year Test of Time Award in 2022 and Best Paper Selection/Awards in VLDB 2000 and 2013 and ASONAM 2019, NSF CAREER Award in 2002, NCSA Faculty Fellow Award in 2003, IBM Faculty Awards in 2004 and 2005, Academy for Entrepreneurial Leadership Faculty Fellow Award in 2008, and the Incomplete List of Excellent Teachers at University of Illinois in 2001, 2004, 2005, 2006, 2010, 2011, 2019, 2022, 2023. He is passionate to bring research results to the real world and, with his students, co-founded Cazoodle, a startup from the University of Illinois, and developed GrantForward.com funding discovery and dissemination service, a vertical search engine integrating 20,000 sources, subscribed by 200+ institutions including Harvard, Stanford, Yale, Cornell University, University of California, CMU, Mayo Clinic, and National Institutes of Health.',\n",
       " 'Professional Highlights\\n- PC Members, SIGMOD, VLDB, ICDE, KDD, EDBT, ICDM, WWW, ASONAM, SIGIR, WSDM, CIKM, AAAI, Recent years.\\n- Area Chair, NeurIPS 2023, 2023.\\n- Associate Editor, Proceedings of the VLDB Endowment (PVLDB), 2014 - 2015.\\n- Area Editor, Encyclopedia of Database Systems, 2014 - 2016.\\n- Workshop Co-chair, 31st IEEE International Conference on Data Engineering (ICDE 2015), 2014 - 2015.\\n- Associate Editor, IEEE Transactions on Knowledge and Data Engineering (TKDE), 2013 - 2017.\\n- Workshop Co-chair, 22rd International World Wide Web Conference (WWW 2014), 2013 - 2014.\\n- PC Co-chair, Track “Bringing Unstructured and Structured Data”, 2012 - 2013.\\n- Best Paper Award Committee, ACM International Conference on Web Search and Data Mining, 2012.\\n- Senior PC, ACM International Conference on Web Search and Data Mining, 2011.\\n- Co-chair, Demonstration Track, ICDE 2011, 2011.\\n- Senior PC, ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2010.\\n- Area Editor, Encyclopedia of Database Systems, 2007 - 2009.\\n- Workshop Chair, APWeb 2007, 2007.\\n- Steering Committee, International Workshop on Information Integration on the Web (IIWeb 2007) at AAAI, 2007.\\n- Workshop Chair, ACM SIGMOD 2006 Conference, 2006.\\n- Co-chair, International Workshop on Information Integration on the Web (IIWeb 2006) at WWW, 2006.\\n- Co-chair, International Workshop on Challenges in Web Information Retrieval and Integration (WIRI 2006) at ICDE, 2006.\\n- Guest Editor, SIGKDD Explorations 6(2) Special Issue on Web Content Mining, 2004.\\n- Chair, NSF DIMACS Center Tutorial/Summer School on Social Choice and Computer Science, 2004.',\n",
       " 'Research Statement\\nI lead the FORWARD Data Lab group, which is part of the larger Data and Information Systems Laboratories, at the CS department of UIUC. Our research overall aims at bridging\\n*structured* and *unstructured data*— to bring structured/semantic-rich access to the myriad and massive unstructured data which accounts for most of the world’s information. Therefore, our research spans *natural language processing*, *data mining*, *data management/databases*, *information retrieval*, and *machine learning*. As our objectives, we aim at developing novel systems, principled algorithms, and formal theories that ultimately deliver real-world applications. As our approaches, we seek to be inspired by and learn from the data we are tackling-- i.e., we believe the key to tame big data is to learn the wisdom hidden in the large scale of the data.',\n",
       " 'Research Interests\\n- Natural language processing, data mining, data management, and information retrieval with machine learning techniques, with emphasis on the application areas of Web and social media-based knowledge acquisition and organization across structured and unstructured data.',\n",
       " 'Research Areas',\n",
       " \"Articles in Conference Proceedings\\n- Are Large Pre-Trained Language Models Leaking Your Personal Information?. Jie Huang, Hanyin Shao, Kevin Chen-Chuan Chang. In Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, 2022.\\n- Coordinated Topic Modeling. Pritom Saha Akash, Jie Huang, Kevin Chen-Chuan Chang. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, 2022.\\n- DEER: Descriptive Knowledge Graph for Explaining Entity Relationships. Jie Huang, Kerui Zhu, Kevin Chen-Chuan Chang, Jinjun Xiong, Wen-Mei Hwu. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, 2022.\\n- Understanding Jargon: Combining Extraction and Generation for Definition Modeling. Jie Huang, Hanyin Shao, Kevin Chen-Chuan Chang, Jinjun Xiong, Wen-Mei Hwu. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, 2022.\\n- Unified and Incremental SimRank: Index-Free Approximation With Scheduled Principle (Extended Abstract). Fanwei Zhu, Yuan Fang, Kai Zhang, Kevin Chen-Chuan Chang, Hongtai Cao, Zhen Jiang, Minghui Wu. In 38th IEEE International Conference on Data Engineering, ICDE 2022, Kuala Lumpur, Malaysia, May 9-12, 2022, 2022.\\n- Open Relation Modeling: Learning to Define Relations Between Entities. Jie Huang, Kevin Chen-Chuan Chang, Jinjun Xiong, Wen-Mei Hwu. In Findings of the Association for Computational Linguistics: ACL 2022, Dublin, Ireland, May 22-27, 2022, 2022.\\n- Domain Representative Keywords Selection: A Probabilistic Approach. Pritom Saha Akash, Jie Huang, Kevin Chen-Chuan Chang, Yunyao Li, Lucian Popa, ChengXiang Zhai. In Findings of the Association for Computational Linguistics: ACL 2022, Dublin, Ireland, May 22-27, 2022, 2022.\\n- Measuring Fine-Grained Domain Relevance of Terms: A Hierarchical Core-Fringe Approach. Jie Huang, Kevin Chang, Jinjun Xiong, Wen-Mei Hwu. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, 2021.\\n- On Analyzing Graphs With Motif-Paths. Xiaodong Li, Reynold Cheng, Kevin Chen-Chuan Chang, Caihua Shan, Chenhao Ma, Hongtai Cao. In VLDB 2021, 2021.\\n- MC-Explorer: Analyzing and Visualizing Motif-Cliques on Large Networks. Boxuan Li, Reynold Cheng, Jiafeng Hu, Yixiang Fang, Min Ou, Ruibang Luo, Kevin Chen-Chuan Chang, Xuemin Lin. In 36th IEEE International Conference on Data Engineering, ICDE 2020, Dallas, TX, USA, April 20-24, 2020, 2020.\\n- Geom-GCN: Geometric Graph Convolutional Networks. Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, Bo Yang. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020.\\n- ROSE: Role-Based Signed Network Embedding. Amin Javari, Tyler Derr, Pouya Esmailian, Jiliang Tang, Kevin Chen-Chuan Chang. In WWW '20: The Web Conference 2020, Taipei, Taiwan, April 20-24, 2020, 2020.\\n- Weakly Supervised Attention for Hashtag Recommendation Using Graph Data. Amin Javari, Zhankui He, Zijie Huang, Jeetu Raj, Kevin Chen-Chuan Chang. In WWW '20: The Web Conference 2020, Taipei, Taiwan, April 20-24, 2020, 2020.\\n- M-Cypher: A GQL Framework Supporting Motifs. Xiaodong Li, Reynold Cheng, Matin Najafi, Kevin Chen-Chuan Chang, Xiaolin Han, Hongtai Cao. In CIKM '20: The 29th ACM International Conference on Information and Knowledge Management, Virtual Event, Ireland, October 19-23, 2020, 2020.\\n- GraphEBM: Energy-based Graph Construction for Semi-Supervised Learning. Zhijie Chen, Hongtai Cao,, Kevin Chen-Chuan Chang. In ICDM 2020, 2020.\\n- Exploring Semantic Capacity of Terms. Jie Huang, Zilong Wang, Kevin Chang, Wen-Mei Hwu, Jinjun Xiong. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, 2020.\\n- Curvature Regularization to Prevent Distortion in Graph Embedding. Hongbin Pei, Bingzhe Wei, Kevin Chang, Chunxu Zhang, Bo Yang. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\",\n",
       " 'Teaching Honors\\n- UIUC List of Teachers Ranked as Excellent by Their Students, Fall 2001, Spring 2004, Fall 2005, Spring 2006, Fall 2010, Fall 2011, Fall 2019, Spring 2022, Spring 2023.',\n",
       " 'Research Honors\\n- ICDE Influential Paper (10-Year Test of Time) Award, IEEE Computer Society, 2022.\\n- Best Demo Award,IEEE International Conference on Data Engineering (ICDE) 2019.\\n- Best Paper Award, International Conference on Advances in Social Networks Analysis and Mining (ASONAM), 2019.\\n- Best-Papers Selections of Very Large Data Bases (VLDB) 2013.\\n- Academy of Entrepreneurial Leadership Faculty Fellow Award, 2008.\\n- IBM Faculty Award, 2005.\\n- IBM Faculty Award, 2004.\\n- NCSA (National Center for Supercomputing Applications) Faculty Fellows Award, 2003.\\n- National Science Foundation CAREER Award 2002.\\n- Best-Papers Selections of Very Large Data Bases (VLDB) 2000.\\n- Philips FMA Fellowships, 1996-1998.',\n",
       " 'Recent Courses Taught\\n- CS 411 - Database Systems\\n- CS 412 CSP - Introduction to Data Mining\\n- CS 511 - Advanced Data Management\\n- CS 598 KCC (CS 598 KCO) - Listening to Social Universe',\n",
       " 'Kevin Chenchuan Chang',\n",
       " 'For More Information',\n",
       " 'Education\\n- Ph.D. Electrial Engineering, Stanford University, 2001',\n",
       " 'Biography\\nKevin Chen-Chuan Chang is a Professor in Computer Science, University of Illinois at Urbana-Champaign. He received a BS from National Taiwan University and PhD from Stanford University in Electrical Engineering. His research addresses large-scale information access and knowledge acquisition, for search, mining, and integration across structured and unstructured big data, with current focuses on Web search/mining and social media analytics. He received ICDE 10-Year Test of Time Award in 2022 and Best Paper Selection/Awards in VLDB 2000 and 2013 and ASONAM 2019, NSF CAREER Award in 2002, NCSA Faculty Fellow Award in 2003, IBM Faculty Awards in 2004 and 2005, Academy for Entrepreneurial Leadership Faculty Fellow Award in 2008, and the Incomplete List of Excellent Teachers at University of Illinois in 2001, 2004, 2005, 2006, 2010, 2011, 2019, 2022, 2023. He is passionate to bring research results to the real world and, with his students, co-founded Cazoodle, a startup from the University of Illinois, and developed GrantForward.com funding discovery and dissemination service, a vertical search engine integrating 20,000 sources, subscribed by 200+ institutions including Harvard, Stanford, Yale, Cornell University, University of California, CMU, Mayo Clinic, and National Institutes of Health.',\n",
       " 'Professional Highlights\\n- PC Members, SIGMOD, VLDB, ICDE, KDD, EDBT, ICDM, WWW, ASONAM, SIGIR, WSDM, CIKM, AAAI, Recent years.\\n- Area Chair, NeurIPS 2023, 2023.\\n- Associate Editor, Proceedings of the VLDB Endowment (PVLDB), 2014 - 2015.\\n- Area Editor, Encyclopedia of Database Systems, 2014 - 2016.\\n- Workshop Co-chair, 31st IEEE International Conference on Data Engineering (ICDE 2015), 2014 - 2015.\\n- Associate Editor, IEEE Transactions on Knowledge and Data Engineering (TKDE), 2013 - 2017.\\n- Workshop Co-chair, 22rd International World Wide Web Conference (WWW 2014), 2013 - 2014.\\n- PC Co-chair, Track “Bringing Unstructured and Structured Data”, 2012 - 2013.\\n- Best Paper Award Committee, ACM International Conference on Web Search and Data Mining, 2012.\\n- Senior PC, ACM International Conference on Web Search and Data Mining, 2011.\\n- Co-chair, Demonstration Track, ICDE 2011, 2011.\\n- Senior PC, ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2010.\\n- Area Editor, Encyclopedia of Database Systems, 2007 - 2009.\\n- Workshop Chair, APWeb 2007, 2007.\\n- Steering Committee, International Workshop on Information Integration on the Web (IIWeb 2007) at AAAI, 2007.\\n- Workshop Chair, ACM SIGMOD 2006 Conference, 2006.\\n- Co-chair, International Workshop on Information Integration on the Web (IIWeb 2006) at WWW, 2006.\\n- Co-chair, International Workshop on Challenges in Web Information Retrieval and Integration (WIRI 2006) at ICDE, 2006.\\n- Guest Editor, SIGKDD Explorations 6(2) Special Issue on Web Content Mining, 2004.\\n- Chair, NSF DIMACS Center Tutorial/Summer School on Social Choice and Computer Science, 2004.',\n",
       " 'Research Statement\\nI lead the FORWARD Data Lab group, which is part of the larger Data and Information Systems Laboratories, at the CS department of UIUC. Our research overall aims at bridging\\n*structured* and *unstructured data*— to bring structured/semantic-rich access to the myriad and massive unstructured data which accounts for most of the world’s information. Therefore, our research spans *natural language processing*, *data mining*, *data management/databases*, *information retrieval*, and *machine learning*. As our objectives, we aim at developing novel systems, principled algorithms, and formal theories that ultimately deliver real-world applications. As our approaches, we seek to be inspired by and learn from the data we are tackling-- i.e., we believe the key to tame big data is to learn the wisdom hidden in the large scale of the data.',\n",
       " 'Research Interests\\n- Natural language processing, data mining, data management, and information retrieval with machine learning techniques, with emphasis on the application areas of Web and social media-based knowledge acquisition and organization across structured and unstructured data.',\n",
       " 'Research Areas',\n",
       " \"Articles in Conference Proceedings\\n- Are Large Pre-Trained Language Models Leaking Your Personal Information?. Jie Huang, Hanyin Shao, Kevin Chen-Chuan Chang. In Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, 2022.\\n- Coordinated Topic Modeling. Pritom Saha Akash, Jie Huang, Kevin Chen-Chuan Chang. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, 2022.\\n- DEER: Descriptive Knowledge Graph for Explaining Entity Relationships. Jie Huang, Kerui Zhu, Kevin Chen-Chuan Chang, Jinjun Xiong, Wen-Mei Hwu. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, 2022.\\n- Understanding Jargon: Combining Extraction and Generation for Definition Modeling. Jie Huang, Hanyin Shao, Kevin Chen-Chuan Chang, Jinjun Xiong, Wen-Mei Hwu. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, 2022.\\n- Unified and Incremental SimRank: Index-Free Approximation With Scheduled Principle (Extended Abstract). Fanwei Zhu, Yuan Fang, Kai Zhang, Kevin Chen-Chuan Chang, Hongtai Cao, Zhen Jiang, Minghui Wu. In 38th IEEE International Conference on Data Engineering, ICDE 2022, Kuala Lumpur, Malaysia, May 9-12, 2022, 2022.\\n- Open Relation Modeling: Learning to Define Relations Between Entities. Jie Huang, Kevin Chen-Chuan Chang, Jinjun Xiong, Wen-Mei Hwu. In Findings of the Association for Computational Linguistics: ACL 2022, Dublin, Ireland, May 22-27, 2022, 2022.\\n- Domain Representative Keywords Selection: A Probabilistic Approach. Pritom Saha Akash, Jie Huang, Kevin Chen-Chuan Chang, Yunyao Li, Lucian Popa, ChengXiang Zhai. In Findings of the Association for Computational Linguistics: ACL 2022, Dublin, Ireland, May 22-27, 2022, 2022.\\n- Measuring Fine-Grained Domain Relevance of Terms: A Hierarchical Core-Fringe Approach. Jie Huang, Kevin Chang, Jinjun Xiong, Wen-Mei Hwu. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, 2021.\\n- On Analyzing Graphs With Motif-Paths. Xiaodong Li, Reynold Cheng, Kevin Chen-Chuan Chang, Caihua Shan, Chenhao Ma, Hongtai Cao. In VLDB 2021, 2021.\\n- MC-Explorer: Analyzing and Visualizing Motif-Cliques on Large Networks. Boxuan Li, Reynold Cheng, Jiafeng Hu, Yixiang Fang, Min Ou, Ruibang Luo, Kevin Chen-Chuan Chang, Xuemin Lin. In 36th IEEE International Conference on Data Engineering, ICDE 2020, Dallas, TX, USA, April 20-24, 2020, 2020.\\n- Geom-GCN: Geometric Graph Convolutional Networks. Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, Bo Yang. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020.\\n- ROSE: Role-Based Signed Network Embedding. Amin Javari, Tyler Derr, Pouya Esmailian, Jiliang Tang, Kevin Chen-Chuan Chang. In WWW '20: The Web Conference 2020, Taipei, Taiwan, April 20-24, 2020, 2020.\\n- Weakly Supervised Attention for Hashtag Recommendation Using Graph Data. Amin Javari, Zhankui He, Zijie Huang, Jeetu Raj, Kevin Chen-Chuan Chang. In WWW '20: The Web Conference 2020, Taipei, Taiwan, April 20-24, 2020, 2020.\\n- M-Cypher: A GQL Framework Supporting Motifs. Xiaodong Li, Reynold Cheng, Matin Najafi, Kevin Chen-Chuan Chang, Xiaolin Han, Hongtai Cao. In CIKM '20: The 29th ACM International Conference on Information and Knowledge Management, Virtual Event, Ireland, October 19-23, 2020, 2020.\\n- GraphEBM: Energy-based Graph Construction for Semi-Supervised Learning. Zhijie Chen, Hongtai Cao,, Kevin Chen-Chuan Chang. In ICDM 2020, 2020.\\n- Exploring Semantic Capacity of Terms. Jie Huang, Zilong Wang, Kevin Chang, Wen-Mei Hwu, Jinjun Xiong. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, 2020.\\n- Curvature Regularization to Prevent Distortion in Graph Embedding. Hongbin Pei, Bingzhe Wei, Kevin Chang, Chunxu Zhang, Bo Yang. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\",\n",
       " 'Teaching Honors\\n- UIUC List of Teachers Ranked as Excellent by Their Students, Fall 2001, Spring 2004, Fall 2005, Spring 2006, Fall 2010, Fall 2011, Fall 2019, Spring 2022, Spring 2023.',\n",
       " 'Research Honors\\n- ICDE Influential Paper (10-Year Test of Time) Award, IEEE Computer Society, 2022.\\n- Best Demo Award,IEEE International Conference on Data Engineering (ICDE) 2019.\\n- Best Paper Award, International Conference on Advances in Social Networks Analysis and Mining (ASONAM), 2019.\\n- Best-Papers Selections of Very Large Data Bases (VLDB) 2013.\\n- Academy of Entrepreneurial Leadership Faculty Fellow Award, 2008.\\n- IBM Faculty Award, 2005.\\n- IBM Faculty Award, 2004.\\n- NCSA (National Center for Supercomputing Applications) Faculty Fellows Award, 2003.\\n- National Science Foundation CAREER Award 2002.\\n- Best-Papers Selections of Very Large Data Bases (VLDB) 2000.\\n- Philips FMA Fellowships, 1996-1998.',\n",
       " 'Recent Courses Taught\\n- CS 411 - Database Systems\\n- CS 412 CSP - Introduction to Data Mining\\n- CS 511 - Advanced Data Management\\n- CS 598 KCC (CS 598 KCO) - Listening to Social Universe',\n",
       " 'Kevin Chenchuan Chang',\n",
       " 'For More Information',\n",
       " 'Education\\n- Ph.D. Electrial Engineering, Stanford University, 2001',\n",
       " 'Biography\\nKevin Chen-Chuan Chang is a Professor in Computer Science, University of Illinois at Urbana-Champaign. He received a BS from National Taiwan University and PhD from Stanford University in Electrical Engineering. His research addresses large-scale information access and knowledge acquisition, for search, mining, and integration across structured and unstructured big data, with current focuses on Web search/mining and social media analytics. He received ICDE 10-Year Test of Time Award in 2022 and Best Paper Selection/Awards in VLDB 2000 and 2013 and ASONAM 2019, NSF CAREER Award in 2002, NCSA Faculty Fellow Award in 2003, IBM Faculty Awards in 2004 and 2005, Academy for Entrepreneurial Leadership Faculty Fellow Award in 2008, and the Incomplete List of Excellent Teachers at University of Illinois in 2001, 2004, 2005, 2006, 2010, 2011, 2019, 2022, 2023. He is passionate to bring research results to the real world and, with his students, co-founded Cazoodle, a startup from the University of Illinois, and developed GrantForward.com funding discovery and dissemination service, a vertical search engine integrating 20,000 sources, subscribed by 200+ institutions including Harvard, Stanford, Yale, Cornell University, University of California, CMU, Mayo Clinic, and National Institutes of Health.',\n",
       " 'Professional Highlights\\n- PC Members, SIGMOD, VLDB, ICDE, KDD, EDBT, ICDM, WWW, ASONAM, SIGIR, WSDM, CIKM, AAAI, Recent years.\\n- Area Chair, NeurIPS 2023, 2023.\\n- Associate Editor, Proceedings of the VLDB Endowment (PVLDB), 2014 - 2015.\\n- Area Editor, Encyclopedia of Database Systems, 2014 - 2016.\\n- Workshop Co-chair, 31st IEEE International Conference on Data Engineering (ICDE 2015), 2014 - 2015.\\n- Associate Editor, IEEE Transactions on Knowledge and Data Engineering (TKDE), 2013 - 2017.\\n- Workshop Co-chair, 22rd International World Wide Web Conference (WWW 2014), 2013 - 2014.\\n- PC Co-chair, Track “Bringing Unstructured and Structured Data”, 2012 - 2013.\\n- Best Paper Award Committee, ACM International Conference on Web Search and Data Mining, 2012.\\n- Senior PC, ACM International Conference on Web Search and Data Mining, 2011.\\n- Co-chair, Demonstration Track, ICDE 2011, 2011.\\n- Senior PC, ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2010.\\n- Area Editor, Encyclopedia of Database Systems, 2007 - 2009.\\n- Workshop Chair, APWeb 2007, 2007.\\n- Steering Committee, International Workshop on Information Integration on the Web (IIWeb 2007) at AAAI, 2007.\\n- Workshop Chair, ACM SIGMOD 2006 Conference, 2006.\\n- Co-chair, International Workshop on Information Integration on the Web (IIWeb 2006) at WWW, 2006.\\n- Co-chair, International Workshop on Challenges in Web Information Retrieval and Integration (WIRI 2006) at ICDE, 2006.\\n- Guest Editor, SIGKDD Explorations 6(2) Special Issue on Web Content Mining, 2004.\\n- Chair, NSF DIMACS Center Tutorial/Summer School on Social Choice and Computer Science, 2004.',\n",
       " 'Research Statement\\nI lead the FORWARD Data Lab group, which is part of the larger Data and Information Systems Laboratories, at the CS department of UIUC. Our research overall aims at bridging\\n*structured* and *unstructured data*— to bring structured/semantic-rich access to the myriad and massive unstructured data which accounts for most of the world’s information. Therefore, our research spans *natural language processing*, *data mining*, *data management/databases*, *information retrieval*, and *machine learning*. As our objectives, we aim at developing novel systems, principled algorithms, and formal theories that ultimately deliver real-world applications. As our approaches, we seek to be inspired by and learn from the data we are tackling-- i.e., we believe the key to tame big data is to learn the wisdom hidden in the large scale of the data.',\n",
       " 'Research Interests\\n- Natural language processing, data mining, data management, and information retrieval with machine learning techniques, with emphasis on the application areas of Web and social media-based knowledge acquisition and organization across structured and unstructured data.',\n",
       " 'Research Areas',\n",
       " \"Articles in Conference Proceedings\\n- Are Large Pre-Trained Language Models Leaking Your Personal Information?. Jie Huang, Hanyin Shao, Kevin Chen-Chuan Chang. In Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, 2022.\\n- Coordinated Topic Modeling. Pritom Saha Akash, Jie Huang, Kevin Chen-Chuan Chang. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, 2022.\\n- DEER: Descriptive Knowledge Graph for Explaining Entity Relationships. Jie Huang, Kerui Zhu, Kevin Chen-Chuan Chang, Jinjun Xiong, Wen-Mei Hwu. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, 2022.\\n- Understanding Jargon: Combining Extraction and Generation for Definition Modeling. Jie Huang, Hanyin Shao, Kevin Chen-Chuan Chang, Jinjun Xiong, Wen-Mei Hwu. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, 2022.\\n- Unified and Incremental SimRank: Index-Free Approximation With Scheduled Principle (Extended Abstract). Fanwei Zhu, Yuan Fang, Kai Zhang, Kevin Chen-Chuan Chang, Hongtai Cao, Zhen Jiang, Minghui Wu. In 38th IEEE International Conference on Data Engineering, ICDE 2022, Kuala Lumpur, Malaysia, May 9-12, 2022, 2022.\\n- Open Relation Modeling: Learning to Define Relations Between Entities. Jie Huang, Kevin Chen-Chuan Chang, Jinjun Xiong, Wen-Mei Hwu. In Findings of the Association for Computational Linguistics: ACL 2022, Dublin, Ireland, May 22-27, 2022, 2022.\\n- Domain Representative Keywords Selection: A Probabilistic Approach. Pritom Saha Akash, Jie Huang, Kevin Chen-Chuan Chang, Yunyao Li, Lucian Popa, ChengXiang Zhai. In Findings of the Association for Computational Linguistics: ACL 2022, Dublin, Ireland, May 22-27, 2022, 2022.\\n- Measuring Fine-Grained Domain Relevance of Terms: A Hierarchical Core-Fringe Approach. Jie Huang, Kevin Chang, Jinjun Xiong, Wen-Mei Hwu. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, 2021.\\n- On Analyzing Graphs With Motif-Paths. Xiaodong Li, Reynold Cheng, Kevin Chen-Chuan Chang, Caihua Shan, Chenhao Ma, Hongtai Cao. In VLDB 2021, 2021.\\n- MC-Explorer: Analyzing and Visualizing Motif-Cliques on Large Networks. Boxuan Li, Reynold Cheng, Jiafeng Hu, Yixiang Fang, Min Ou, Ruibang Luo, Kevin Chen-Chuan Chang, Xuemin Lin. In 36th IEEE International Conference on Data Engineering, ICDE 2020, Dallas, TX, USA, April 20-24, 2020, 2020.\\n- Geom-GCN: Geometric Graph Convolutional Networks. Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, Bo Yang. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020.\\n- ROSE: Role-Based Signed Network Embedding. Amin Javari, Tyler Derr, Pouya Esmailian, Jiliang Tang, Kevin Chen-Chuan Chang. In WWW '20: The Web Conference 2020, Taipei, Taiwan, April 20-24, 2020, 2020.\\n- Weakly Supervised Attention for Hashtag Recommendation Using Graph Data. Amin Javari, Zhankui He, Zijie Huang, Jeetu Raj, Kevin Chen-Chuan Chang. In WWW '20: The Web Conference 2020, Taipei, Taiwan, April 20-24, 2020, 2020.\\n- M-Cypher: A GQL Framework Supporting Motifs. Xiaodong Li, Reynold Cheng, Matin Najafi, Kevin Chen-Chuan Chang, Xiaolin Han, Hongtai Cao. In CIKM '20: The 29th ACM International Conference on Information and Knowledge Management, Virtual Event, Ireland, October 19-23, 2020, 2020.\\n- GraphEBM: Energy-based Graph Construction for Semi-Supervised Learning. Zhijie Chen, Hongtai Cao,, Kevin Chen-Chuan Chang. In ICDM 2020, 2020.\\n- Exploring Semantic Capacity of Terms. Jie Huang, Zilong Wang, Kevin Chang, Wen-Mei Hwu, Jinjun Xiong. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, 2020.\\n- Curvature Regularization to Prevent Distortion in Graph Embedding. Hongbin Pei, Bingzhe Wei, Kevin Chang, Chunxu Zhang, Bo Yang. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\",\n",
       " 'Teaching Honors\\n- UIUC List of Teachers Ranked as Excellent by Their Students, Fall 2001, Spring 2004, Fall 2005, Spring 2006, Fall 2010, Fall 2011, Fall 2019, Spring 2022, Spring 2023.',\n",
       " 'Research Honors\\n- ICDE Influential Paper (10-Year Test of Time) Award, IEEE Computer Society, 2022.\\n- Best Demo Award,IEEE International Conference on Data Engineering (ICDE) 2019.\\n- Best Paper Award, International Conference on Advances in Social Networks Analysis and Mining (ASONAM), 2019.\\n- Best-Papers Selections of Very Large Data Bases (VLDB) 2013.\\n- Academy of Entrepreneurial Leadership Faculty Fellow Award, 2008.\\n- IBM Faculty Award, 2005.\\n- IBM Faculty Award, 2004.\\n- NCSA (National Center for Supercomputing Applications) Faculty Fellows Award, 2003.\\n- National Science Foundation CAREER Award 2002.\\n- Best-Papers Selections of Very Large Data Bases (VLDB) 2000.\\n- Philips FMA Fellowships, 1996-1998.',\n",
       " 'Recent Courses Taught\\n- CS 411 - Database Systems\\n- CS 412 CSP - Introduction to Data Mining\\n- CS 511 - Advanced Data Management\\n- CS 598 KCC (CS 598 KCO) - Listening to Social Universe',\n",
       " 'Kevin Chenchuan Chang',\n",
       " 'For More Information',\n",
       " 'Education\\n- Ph.D. Electrial Engineering, Stanford University, 2001',\n",
       " 'Biography\\nKevin Chen-Chuan Chang is a Professor in Computer Science, University of Illinois at Urbana-Champaign. He received a BS from National Taiwan University and PhD from Stanford University in Electrical Engineering. His research addresses large-scale information access and knowledge acquisition, for search, mining, and integration across structured and unstructured big data, with current focuses on Web search/mining and social media analytics. He received ICDE 10-Year Test of Time Award in 2022 and Best Paper Selection/Awards in VLDB 2000 and 2013 and ASONAM 2019, NSF CAREER Award in 2002, NCSA Faculty Fellow Award in 2003, IBM Faculty Awards in 2004 and 2005, Academy for Entrepreneurial Leadership Faculty Fellow Award in 2008, and the Incomplete List of Excellent Teachers at University of Illinois in 2001, 2004, 2005, 2006, 2010, 2011, 2019, 2022, 2023. He is passionate to bring research results to the real world and, with his students, co-founded Cazoodle, a startup from the University of Illinois, and developed GrantForward.com funding discovery and dissemination service, a vertical search engine integrating 20,000 sources, subscribed by 200+ institutions including Harvard, Stanford, Yale, Cornell University, University of California, CMU, Mayo Clinic, and National Institutes of Health.',\n",
       " 'Professional Highlights\\n- PC Members, SIGMOD, VLDB, ICDE, KDD, EDBT, ICDM, WWW, ASONAM, SIGIR, WSDM, CIKM, AAAI, Recent years.\\n- Area Chair, NeurIPS 2023, 2023.\\n- Associate Editor, Proceedings of the VLDB Endowment (PVLDB), 2014 - 2015.\\n- Area Editor, Encyclopedia of Database Systems, 2014 - 2016.\\n- Workshop Co-chair, 31st IEEE International Conference on Data Engineering (ICDE 2015), 2014 - 2015.\\n- Associate Editor, IEEE Transactions on Knowledge and Data Engineering (TKDE), 2013 - 2017.\\n- Workshop Co-chair, 22rd International World Wide Web Conference (WWW 2014), 2013 - 2014.\\n- PC Co-chair, Track “Bringing Unstructured and Structured Data”, 2012 - 2013.\\n- Best Paper Award Committee, ACM International Conference on Web Search and Data Mining, 2012.\\n- Senior PC, ACM International Conference on Web Search and Data Mining, 2011.\\n- Co-chair, Demonstration Track, ICDE 2011, 2011.\\n- Senior PC, ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2010.\\n- Area Editor, Encyclopedia of Database Systems, 2007 - 2009.\\n- Workshop Chair, APWeb 2007, 2007.\\n- Steering Committee, International Workshop on Information Integration on the Web (IIWeb 2007) at AAAI, 2007.\\n- Workshop Chair, ACM SIGMOD 2006 Conference, 2006.\\n- Co-chair, International Workshop on Information Integration on the Web (IIWeb 2006) at WWW, 2006.\\n- Co-chair, International Workshop on Challenges in Web Information Retrieval and Integration (WIRI 2006) at ICDE, 2006.\\n- Guest Editor, SIGKDD Explorations 6(2) Special Issue on Web Content Mining, 2004.\\n- Chair, NSF DIMACS Center Tutorial/Summer School on Social Choice and Computer Science, 2004.',\n",
       " 'Research Statement\\nI lead the FORWARD Data Lab group, which is part of the larger Data and Information Systems Laboratories, at the CS department of UIUC. Our research overall aims at bridging\\n*structured* and *unstructured data*— to bring structured/semantic-rich access to the myriad and massive unstructured data which accounts for most of the world’s information. Therefore, our research spans *natural language processing*, *data mining*, *data management/databases*, *information retrieval*, and *machine learning*. As our objectives, we aim at developing novel systems, principled algorithms, and formal theories that ultimately deliver real-world applications. As our approaches, we seek to be inspired by and learn from the data we are tackling-- i.e., we believe the key to tame big data is to learn the wisdom hidden in the large scale of the data.',\n",
       " 'Research Interests\\n- Natural language processing, data mining, data management, and information retrieval with machine learning techniques, with emphasis on the application areas of Web and social media-based knowledge acquisition and organization across structured and unstructured data.',\n",
       " 'Research Areas',\n",
       " \"Articles in Conference Proceedings\\n- Are Large Pre-Trained Language Models Leaking Your Personal Information?. Jie Huang, Hanyin Shao, Kevin Chen-Chuan Chang. In Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, 2022.\\n- Coordinated Topic Modeling. Pritom Saha Akash, Jie Huang, Kevin Chen-Chuan Chang. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, 2022.\\n- DEER: Descriptive Knowledge Graph for Explaining Entity Relationships. Jie Huang, Kerui Zhu, Kevin Chen-Chuan Chang, Jinjun Xiong, Wen-Mei Hwu. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, 2022.\\n- Understanding Jargon: Combining Extraction and Generation for Definition Modeling. Jie Huang, Hanyin Shao, Kevin Chen-Chuan Chang, Jinjun Xiong, Wen-Mei Hwu. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, 2022.\\n- Unified and Incremental SimRank: Index-Free Approximation With Scheduled Principle (Extended Abstract). Fanwei Zhu, Yuan Fang, Kai Zhang, Kevin Chen-Chuan Chang, Hongtai Cao, Zhen Jiang, Minghui Wu. In 38th IEEE International Conference on Data Engineering, ICDE 2022, Kuala Lumpur, Malaysia, May 9-12, 2022, 2022.\\n- Open Relation Modeling: Learning to Define Relations Between Entities. Jie Huang, Kevin Chen-Chuan Chang, Jinjun Xiong, Wen-Mei Hwu. In Findings of the Association for Computational Linguistics: ACL 2022, Dublin, Ireland, May 22-27, 2022, 2022.\\n- Domain Representative Keywords Selection: A Probabilistic Approach. Pritom Saha Akash, Jie Huang, Kevin Chen-Chuan Chang, Yunyao Li, Lucian Popa, ChengXiang Zhai. In Findings of the Association for Computational Linguistics: ACL 2022, Dublin, Ireland, May 22-27, 2022, 2022.\\n- Measuring Fine-Grained Domain Relevance of Terms: A Hierarchical Core-Fringe Approach. Jie Huang, Kevin Chang, Jinjun Xiong, Wen-Mei Hwu. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, 2021.\\n- On Analyzing Graphs With Motif-Paths. Xiaodong Li, Reynold Cheng, Kevin Chen-Chuan Chang, Caihua Shan, Chenhao Ma, Hongtai Cao. In VLDB 2021, 2021.\\n- MC-Explorer: Analyzing and Visualizing Motif-Cliques on Large Networks. Boxuan Li, Reynold Cheng, Jiafeng Hu, Yixiang Fang, Min Ou, Ruibang Luo, Kevin Chen-Chuan Chang, Xuemin Lin. In 36th IEEE International Conference on Data Engineering, ICDE 2020, Dallas, TX, USA, April 20-24, 2020, 2020.\\n- Geom-GCN: Geometric Graph Convolutional Networks. Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, Bo Yang. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020.\\n- ROSE: Role-Based Signed Network Embedding. Amin Javari, Tyler Derr, Pouya Esmailian, Jiliang Tang, Kevin Chen-Chuan Chang. In WWW '20: The Web Conference 2020, Taipei, Taiwan, April 20-24, 2020, 2020.\\n- Weakly Supervised Attention for Hashtag Recommendation Using Graph Data. Amin Javari, Zhankui He, Zijie Huang, Jeetu Raj, Kevin Chen-Chuan Chang. In WWW '20: The Web Conference 2020, Taipei, Taiwan, April 20-24, 2020, 2020.\\n- M-Cypher: A GQL Framework Supporting Motifs. Xiaodong Li, Reynold Cheng, Matin Najafi, Kevin Chen-Chuan Chang, Xiaolin Han, Hongtai Cao. In CIKM '20: The 29th ACM International Conference on Information and Knowledge Management, Virtual Event, Ireland, October 19-23, 2020, 2020.\\n- GraphEBM: Energy-based Graph Construction for Semi-Supervised Learning. Zhijie Chen, Hongtai Cao,, Kevin Chen-Chuan Chang. In ICDM 2020, 2020.\\n- Exploring Semantic Capacity of Terms. Jie Huang, Zilong Wang, Kevin Chang, Wen-Mei Hwu, Jinjun Xiong. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, 2020.\\n- Curvature Regularization to Prevent Distortion in Graph Embedding. Hongbin Pei, Bingzhe Wei, Kevin Chang, Chunxu Zhang, Bo Yang. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\",\n",
       " 'Teaching Honors\\n- UIUC List of Teachers Ranked as Excellent by Their Students, Fall 2001, Spring 2004, Fall 2005, Spring 2006, Fall 2010, Fall 2011, Fall 2019, Spring 2022, Spring 2023.',\n",
       " 'Research Honors\\n- ICDE Influential Paper (10-Year Test of Time) Award, IEEE Computer Society, 2022.\\n- Best Demo Award,IEEE International Conference on Data Engineering (ICDE) 2019.\\n- Best Paper Award, International Conference on Advances in Social Networks Analysis and Mining (ASONAM), 2019.\\n- Best-Papers Selections of Very Large Data Bases (VLDB) 2013.\\n- Academy of Entrepreneurial Leadership Faculty Fellow Award, 2008.\\n- IBM Faculty Award, 2005.\\n- IBM Faculty Award, 2004.\\n- NCSA (National Center for Supercomputing Applications) Faculty Fellows Award, 2003.\\n- National Science Foundation CAREER Award 2002.\\n- Best-Papers Selections of Very Large Data Bases (VLDB) 2000.\\n- Philips FMA Fellowships, 1996-1998.',\n",
       " 'Recent Courses Taught\\n- CS 411 - Database Systems\\n- CS 412 CSP - Introduction to Data Mining\\n- CS 511 - Advanced Data Management\\n- CS 598 KCC (CS 598 KCO) - Listening to Social Universe']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def split_sections(text):\n",
    "    ''' given a text split the text into sections where each section starts with some a certain number of hashtags'''\n",
    "\n",
    "    sections = re.split(r'#+', text)\n",
    "    sections = [section.strip() for section in sections if section.strip() != '']\n",
    "    return sections\n",
    "\n",
    "contexts = split_sections(context)\n",
    "contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devgoyal/anaconda3/envs/research/lib/python3.8/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (2048) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  Where did Professor get his PhD?\n",
      "Answer:  <pad> Stanford University</s>\n",
      "\n",
      "\n",
      "Question:  What does the professor teach?\n",
      "Answer:  <pad> Electrial Engineering</s>\n",
      "\n",
      "\n",
      "Question:  Where did professor study from?\n",
      "Answer:  <pad> National Taiwan University</s>\n",
      "\n",
      "\n",
      "Question:  List the recent courses taught?\n",
      "Answer:  <pad> ICDE, APWeb 2007, 2007 - 2009.</s>\n",
      "\n",
      "\n",
      "Question:  What is the professor's research interest?\n",
      "Answer:  <pad> bridging *structured* and *unstructured data*— to bring structured/semantic-rich access to the myriad and massive unstructured data which accounts for most of the world’s information</s>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "def get_answer(question, context):\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "    model_flan = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\",max_length=2048)\n",
    "    input_text = \"Answer the question as precisely as possible based on the context and say unanswerable if you can't find it \\n\\n context: v1 \\n\\n question: v2\" \n",
    "    input_text = input_text.replace('v1', context)\n",
    "    input_text = input_text.replace('v2', question)\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "    outputs = model_flan.generate(input_ids)\n",
    "    return tokenizer.decode(outputs[0])\n",
    "\n",
    "\n",
    "# get the answer\n",
    "for i in contexts:\n",
    "    if len(questions) == 0:\n",
    "        break\n",
    "    for question in questions:\n",
    "        answer = get_answer(question, i)\n",
    "        if answer != \"<pad> unanswerable</s>\":\n",
    "            print(\"Question: \", question)\n",
    "            print(\"Answer: \", answer)\n",
    "            print(\"\\n\")\n",
    "            questions.remove(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the documents folder\n",
    "import os\n",
    "\n",
    "for file in os.listdir('Documents'):\n",
    "    os.remove('Documents/' + file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
